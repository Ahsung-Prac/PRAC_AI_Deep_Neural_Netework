{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aifunc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum\n",
    "# v = a*v - lr * dL/dw\n",
    "# W = W + v  ... v는 누적  v초기값은 0, a는 0.9정도..\n",
    "# v는 누적이면서 점점 0.9씩 작이짐.\n",
    "# 마치 물리 법칙처럼 가속하듯이 누적 but 0.9를 곱하면서 가속도 점점 줄어듬.\n",
    "# 지그제그로 움직이지 않고 부드럽게 움직인다.\n",
    "\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        # 매개변수에 v를 더하면서 나아간다\n",
    "        # v도 기울기를 빼주며 값이 누적된다.\n",
    "        # 원래 값에 0.9씩 곱하며 \n",
    "        # 마치 물리 법칙처럼 가속하듯이 누적 but 0.9를 곱하면서 가속도 점점 줄어듬.\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각의 매개변수 별로 학습률을 조정해 주겠다~~\n",
    "# 학습 더 할 필요 없는 애들은 lr을 낮추고 더 필요한 애는 더 키운다.\n",
    "\n",
    "# h = h + dL/dW  *d/L*d/W  (기울기 원소별 제곱)!\n",
    "# W = W - lr * 1/squ(h) * dL/dW\n",
    "\n",
    "# h에는 계속 기울기의 제곱을 누적\n",
    "# 누적값이 많아질 수록 lr을 낮추는 방식!\n",
    "# h가 너무 작아져 0에 가까워지면 학습이 일어나지 않을 수 있다...\n",
    "# 누적중에 오래된 weight 기울기는 잊어버리게 하면, 문제를 조금 해결할 수 있다. \n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "    # 각각 매개변수마다 학습률의 크기가 맞춰진다.\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        # h라는 변수 추가\n",
    "        # h는 기울기의 제곱의 누적합\n",
    "        # 실제 W는 learning rate / h의 1/2승   즉..h가 클수록 learning rate가 작아진다.\n",
    "        # 각 매백변수별로 lr이 맞춰진다.. 기울기가 크게 움직일 수록 더 세밀하게 움직이게끔 조정\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    ##\n",
    "    # AdaGrad와 moment의 융합\n",
    "    # 점점 스스로를 가속하면서\n",
    "    # lr을 세밀하게 조정해나간다.\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            \n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            # m -> moment 부분\n",
    "            # v -> AdaGrad 부분\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
